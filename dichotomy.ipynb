{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6dcb5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradient_descent import GradientDescent\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49d38c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DichotomyScheduler:\n",
    "    def __init__(self, f, max_lr=1, iters=10, delta=1e-2):\n",
    "        self.f = f\n",
    "        self.max_lr = max_lr\n",
    "        self.iters = iters\n",
    "        self.delta = delta\n",
    "        \n",
    "    def _dichotomy(self, f, a, b):\n",
    "        if not 0 < 2 * self.delta < (b - a):\n",
    "            raise RuntimeError(\"`delta` should be in [0, (a + b) / 2]\")\n",
    "        for iter in range(self.iters):\n",
    "            x1 = (a + b) / 2 - self.delta\n",
    "            x2 = (a + b) / 2 + self.delta\n",
    "            if f(x1) > f(x2):\n",
    "                a = x1\n",
    "            else:\n",
    "                b = x2\n",
    "            if b - a < 2 * self.delta:\n",
    "                break\n",
    "        return (a + b) / 2\n",
    "\n",
    "    def step(self, point, gradient):\n",
    "        return self._dichotomy(lambda lr: self.f(point - lr * gradient), 0, self.max_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7956199",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = [lambda x: x ** 2, \n",
    "      lambda x: np.sin(x) / x, \n",
    "      lambda x: 0.001 * x ** 3 + 0.01 * x ** 2]\n",
    "f_grads = [lambda x: x * 2, \n",
    "           lambda x: (np.cos(x) * x - np.sin(x)) / x ** 2, \n",
    "           lambda x: 0.003 * x ** 2 + 0.02 * x]\n",
    "f_names = ['x^2', 'sin(x)divx', 'x^3-10x^2+x']\n",
    "initial_points = [10, 0.1, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e917a9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for f, f_grad, initial_point, f_name in zip(fs, f_grads, initial_points, f_names):\n",
    "    base_gd = GradientDescent(\n",
    "        function=f, \n",
    "        derivative=f_grad, \n",
    "        iterations=100000, \n",
    "        epsilon=1e-6, \n",
    "        initial_point=np.array([initial_point])\n",
    "    )\n",
    "    base_answer, base_iterations_count = base_gd.optimize()\n",
    "    base_gd.plot_trace(\n",
    "        with_function=True, \n",
    "        from_x=-10, \n",
    "        to_x=10, \n",
    "        title=f'BaseScheduler, function={f_name}, minimum={round(base_answer[0], 4)}, iters={base_iterations_count}',\n",
    "    )\n",
    "\n",
    "    scheduler = DichotomyScheduler(f, iters=10)\n",
    "    dichotomy_gd = GradientDescent(\n",
    "        function=f, derivative=f_grad, \n",
    "        scheduler=scheduler, \n",
    "        iterations=100, \n",
    "        initial_point=np.array([initial_point])\n",
    "    )\n",
    "    dichotomy_answer, dichotomy_iterations_count = dichotomy_gd.optimize()\n",
    "    dichotomy_gd.plot_trace(\n",
    "        with_function=True, \n",
    "        from_x=-10, \n",
    "        to_x=10, \n",
    "        title=f'DichotomyScheduler, function={f_name}, minimum={round(dichotomy_answer[0], 4)}, iters={dichotomy_iterations_count}',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30868fe1",
   "metadata": {},
   "source": [
    "В базовой реализации градиентного спуска с константным learning rate количество вызовов функции градиента совпадает с количеством итераций градиентного спуска. В реализации с использованием метода дихотомии на каждом шаге кроме вычисления функции градиента еще в худшем случае O(число итераций метода дихотомии) раз вызывается сама функция. Из графиков видим, что для сходимости градиентного спуска с помощью дихотомии требуется на 3 порядка меньше итераций, а сама дихотомия запускается в данной реализации всего лишь 10 раз, так что получаем, что градиентный спуск с дихотомией работает как м инимум в 100 раз эффективнее, чем стандартный с точки зрения вычислений минимизируемой функции и ее градиентов."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
